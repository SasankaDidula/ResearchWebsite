<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Patrix, Bootstrap 5 Landing Page</title>
  <link rel="stylesheet" href="assets/css/style.css">
  <link rel="stylesheet" href="assets/css/fontawesome.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox/dist/css/glightbox.min.css" />
  <script src="https://cdn.jsdelivr.net/gh/mcstudios/glightbox/dist/js/glightbox.min.js"></script>
  <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600;700;800;900&display=swap"
    rel="stylesheet">
</head>

<body>
  <!-- ////////////////////////////////////////////////////////////////////////////////////////
                               START SECTION 1 - THE NAVBAR SECTION  
/////////////////////////////////////////////////////////////////////////////////////////////-->
  <nav class="navbar navbar-expand-lg navbar-dark menu shadow fixed-top">
    <div class="container">
      <a class="navbar-brand" href="">
        <img src="images/logo_white.png" alt="logo image" width="100" height="50">
      </a>
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav"
        aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse justify-content-end" id="navbarNav">
        <ul class="navbar-nav">
          <li class="nav-item"><a class="nav-link" aria-current="page" href="#">Home</a></li>
          <li class="nav-item"><a class="nav-link" href="#Intro">Intro</a></li>
          <li class="nav-item"><a class="nav-link" href="#Poster">Poster</a></li>
          <li class="nav-item"><a class="nav-link" href="#Scope">Scope</a></li>
          <li class="nav-item"><a class="nav-link" href="#Milestones">Milestones</a></li>
          <li class="nav-item"><a class="nav-link" href="#Presentations">Presentations</a></li>
          <li class="nav-item"><a class="nav-link" href="#Documentation">Documentation</a></li>
          <li class="nav-item"><a class="nav-link" href="#Team">Team</a></li>
          <li class="nav-item"><a class="nav-link" href="#contact">Contact</a>
          </li>
        </ul>
        <!--
        <button type="button" class="rounded-pill btn-rounded">
          +1 728365413
          <span>
            <i class="fas fa-phone-alt"></i>
          </span>
        </button>
-->
      </div>
    </div>
  </nav>

  <!-- /////////////////////////////////////////////////////////////////////////////////////////////////
                            START SECTION 2 - THE INTRO SECTION  
/////////////////////////////////////////////////////////////////////////////////////////////////////-->

  <section id="home" class="intro-section">
    <div class="container">
      <div class="row align-items-center text-white">
        <!-- START THE CONTENT FOR THE INTRO  -->
        <div class="col-md-6 intros text-start">
          <h1 class="display-2">
            <span class="display-2--intro">Hey!, We are Xinli</span>
            <span class="display-2--description lh-base">
              Xinli is the app that makes you mentally happy
            </span>
          </h1>
          <a href="https://drive.google.com/file/d/11S90jUMJETr5R1iUsuo2lNj5Tsvt66Cb/view?usp=sharing">
          <button type="button" class="rounded-pill btn-rounded">Download APK
            <span><i class="fas fa-arrow-right"></i></span>
          </button></a>
        </div>
        <!-- START THE CONTENT FOR THE VIDEO -->
        <div class="col-md-6 intros text-end">
          <div class="video-box">
            <img src="images/arts/intro-section-illustration.png" alt="video illutration" class="img-fluid">
            <a href="#" class="glightbox position-absolute top-50 start-50 translate-middle">
              <span>
                <i class="fas fa-play-circle"></i>
              </span>
              <span class="border-animation border-animation--border-1"></span>
              <span class="border-animation border-animation--border-2"></span>
            </a>
          </div>
        </div>
      </div>
    </div>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1440 320">
      <path fill="#ffffff" fill-opacity="1"
        d="M0,160L48,176C96,192,192,224,288,208C384,192,480,128,576,133.3C672,139,768,213,864,202.7C960,192,1056,96,1152,74.7C1248,53,1344,107,1392,133.3L1440,160L1440,320L1392,320C1344,320,1248,320,1152,320C1056,320,960,320,864,320C768,320,672,320,576,320C480,320,384,320,288,320C192,320,96,320,48,320L0,320Z">
      </path>
    </svg>
  </section>

  <!-- //////////////////////////////////////////////////////////////////////////////////////////////
                         START SECTION - Introduction 
///////////////////////////////////////////////////////////////////////////////////////////////////-->
  <section id="Intro" class="Intro">
    <div class="container">
      <div class="row text-center">
        <h1 class="display-3 fw-bold">Introduction</h1>
        <div class="heading-line mb-1"></div>
      </div>
      <!-- START THE DESCRIPTION CONTENT  -->
            <p>Xinli is a cross-platform app that works on both Android and iOS. It's a social media app that can identify your emotional state and offers a high degree of confidentiality. You can add your closest/most trustworthy friends to keep an eye on you. Xinli is a personalized software that may offer activities to you based on your current emotional condition. Most significantly, you will be guided by a mentor/doctor.            </p>
    </div>
  </section></br></br>

  
  <!-- //////////////////////////////////////////////////////////////////////////////////////////////
                         START SECTION - Poster 
///////////////////////////////////////////////////////////////////////////////////////////////////-->
<section id="Poster" class="Poster">
  <div class="container">
    <div class="row text-center">
      <h1 class="display-3 fw-bold">Poster</h1>
      <div class="heading-line mb-1"></div>
    </div>
    <!-- START THE DESCRIPTION CONTENT  -->
    <img src="images\Poster_21_22-J 08.png" class="img-fluid" alt="Poster">
  </div>
</section></br></br>

  <!-- //////////////////////////////////////////////////////////////////////////////////////////////
                         START SECTION - Scope
///////////////////////////////////////////////////////////////////////////////////////////////////-->
  <section id="Scope">
    <div class="container">
      <div class="row text-center">
        <h1 class="display-3 fw-bold">Scope</h1>
        <div class="heading-line mb-1"></div>
      </div>

      <div class="m-4">
        <ul class="nav nav-pills mb-3 justify-content-center" id="myTab">
          <li class="nav-item">
            <a href="#Literature_Survey" class="nav-link active" data-bs-toggle="tab">Literature Survey</a>
          </li>
          <li class="nav-item">
            <a href="#Research_Gap" class="nav-link" data-bs-toggle="tab">Research Gap</a>
          </li>
          <li class="nav-item">
            <a href="#Research_Problem" class="nav-link" data-bs-toggle="tab">Research Problem</a>
          </li>
          <li class="nav-item">
            <a href="#Research_Objectives" class="nav-link" data-bs-toggle="tab">Research Objectives</a>
          </li>
          <li class="nav-item">
            <a href="#Methodology" class="nav-link" data-bs-toggle="tab">Methodology</a>
          </li>
          <li class="nav-item">
            <a href="#Technologies" class="nav-link" data-bs-toggle="tab">Tools & Technologies</a>
          </li>
        </ul>
        <div class="tab-content">
          <div class="tab-pane fade show active" id="Literature_Survey">
            <h4 class="display-3--title mt-2">Literature Survey</h4>
            <ul class="nav nav-pills mb-3 justify-content-center" id="myTab2">
              <li class="nav-item">
                <a href="#component1" class="nav-link active" data-bs-toggle="tab">IT18220520</a>
              </li>
              <li class="nav-item">
                <a href="#component2" class="nav-link" data-bs-toggle="tab">IT18257250</a>
              </li>
              <li class="nav-item">
                <a href="#component3" class="nav-link" data-bs-toggle="tab">IT18216424</a>
              </li>
              <li class="nav-item">
                <a href="#component4" class="nav-link" data-bs-toggle="tab">IT18219876</a>
              </li>
            </ul>
            <div class="tab-content">
              <div class="tab-pane fade show active" id="component1">
                <p>Emotional expressions in psychology are observable verbal and nonverbal behaviors that
                  communicate an internal emotional or affective state. Examples of emotional expression are facial
                  movements such as smiling or scowling, or behaviors like crying or laughing. The emotions can
                  be categorized in to 5 categories.</br></br>
                  Perceptions describe a person's initial emotional state. You may call it an emotion that impacts
                  your behavior. When these emotions are used to communicate one's views, they become more
                  basic and primordial. This group of emotions may include grief infatuation and infatuation for a
                  departed loved one. When a mother realizes that her child is happy, she experiences a rush of
                  maternal love. The realization that "my child is dead" may induce mother sadness.</br></br>
                  The next set of emotions governs human behavior. This experience is referred to as a sensation.
                  Unlike the preceding category, these sensations may appear suddenly. This section includes
                  everything from hunger and repulsion to sexual pleasure and passion. The taste of food influences
                  its potential to create a pleasurable emotional reaction. Hunger results from a lack of food, which
                  is a negative response. Because of its foul odor, feces may elicit revulsion.</br></br>
                  Reflexes are classified as the third category. Emotions like this one assist us in avoiding danger.
                  Startling horror is a nice example of this. Reflexes are triggered by conclusions or sensory inputs.
                  Certain individuals may be terrified by the conclusion that "a woman is coming at me with a
                  sword." The sheer sight of a tiger has the power to frighten everyone.</br></br>
                  This is the fourth kind of emotion that may be identified from an involuntary expression on the
                  face. As the phrase implies, affect that is exhibited subconsciously or without much thought. In
                  this category, we may include astonishment, tears, extended grins, and flushing. Involuntary
                  utterances might be triggered by a concept, a mood, or a reflex. Fear causes the instinctive
                  manifestation of panic. The sensation of being in love causes the involuntary display of flushing.</br></br>
                  Voluntary expressions are the ultimate form of emotion. In our everyday lives, we utilize these
                  terms to directly transmit our feelings. Anger and laughter are the most typical emotions of this
                  sort. Habitual determination causes voluntary expressiveness. Anger is a regular reaction to a
                  desire for vengeance. Laughter is a regular reaction to comedy. These expressions seem automatic
                  because, like walking or talking, they are deeply entrenched habits.</br></br>
                  As a result of these research, we may conclude that the mouth and its surrounding area, as well as
                  the eyes, are the most important parts of face identification. Because they serve as the foundation
                  for many of the aforementioned feelings. When it comes to identifying emotions, there is no onesize-fits-all solution. There will be a plethora of new solutions that are more efficient, accurate,
                  and user-friendly in the near future. Despite these advancements, no app for mental health therapy
                  can correctly assess a user's mood. We believe that utilizing the same technique stated above, we
                  may be able to address the challenge of emotion recognition using facial traits.</p>
              </div>
              <div class="tab-pane fade show" id="component2">
                <p>The system was found to be capable of distinguishing six various emotions via the use of three distinct methodologies based on facial expression, speech model, and bimodal input (happy, sorrow, disgust, fear, rage, and surprise). Video snippets from the ENTERFACE'05 conference were utilized as inspiration. There are 44 distinct themes available, each with its own distinct vibe. Due to the nonlinear nature of the audio and visual data, the data were classified using a nonlinear multiclass SVM. The polynomial and RBF kernels surpassed all other kernels, accounting for 94.765 and 95.415 percent of the total points, respectively. </br></br> 
 
					In order to create a research model, a total of nine students were educated with negative and neutral emotions using an average of 33 words per student (resulting in 594 sentences in total). There were 396 statements in the training set, six of which expressed negative emotions and six of which expressed neutral feelings. A total of six students had negative feelings and six students had neutral feelings towards the situation. After 198 sentences, the test set, which included three students who felt negatively and three students who felt neutrally, was complete. Training model accuracy was 78%, the test set's accuracy was 70%, and the final model's experimental outcomes were more than 75% accurate, according to the test findings. </br></br> 
 
					Specifically, the goal of this project was to build attention-based 3-D convolution recurrent neural networks for the identification of emotional expressions in speech. To better capture the timefrequency connection between log-Mels, they developed a 3-D convolutional neural network (CRNN) for SER [11].They provide the confusion matrix to aid researchers in comprehending the 3-D ACRNN's SER performance. The IEMOCAP and Emo-DB datasets are used to demonstrate that sad has the greatest recognition rate, whereas happy has the lowest recognition rate. According to IEMOCAP and Emo-DB data, 48.64 percent of happy samples are improperly classified as neutral, whereas 51.43 percent of joyful samples are incorrectly classified as furious. Despite the fact that furious and joyous have almost comparable activation levels, neutral is placed in the middle of the activation-valence space, which the researchers believe explains for the discrepancies. </br></br> </p>
              </div>
              <div class="tab-pane fade show" id="component3">
                <p>Recent research reveals that linguistic features are frequently extracted using neural network
                  algorithms such as Word2Vec, FastText , BERT , and ELMO. These methods
                  have several advantages, including the ability to vectorize words while considering the context of
                  the text and the semantic proximity of words, in contrast to simple algorithms such as Bag-ofWords and One-hot Encoding [16], which consider the occurrence and frequency of the word
                  while ignoring the grammar and word order in the sentence .One of these benefits is the ability to vectorize words while also taking into consideration
				  the context of the text and the semantic proximity of words.Emotion recognition in text is a key area of natural language processing
					(NLP) research, with applications ranging from data mining to e-learning,
					information filtering systems, human–computer interaction, and psychology.
					The issue that has received the most attention in the literature is explicit
					emotion identification in text. Most of the this problem's resolution is
					determined by keyword identification. </br></br>
                  Numerous studies have been published on the evaluation of speech to text performance, in which
                  authors investigate best practices and propose objective, comparable performance standards,
                  Along with typical laboratory circumstances (quiet environment, wideband, and read
                  speech), speech to text has been studied in several non-standard and adverse environments.
				  A different technique [29] uses a novel LSTM architecture to categorize entire social media conversations with
					emotion labels. This method, on the other hand, is tailored to the particular
					qualities and emotions of this form of conversational data [30].
                  Automated transcribing reviews, for example, are available.in noisy environments  (e.g.,
                  traffic, crowd),</br></br>
                  • with foreign accents ,</br>
                  • with children's voice tone, </br>
                  • and with subjects presenting disfluencies in speech and with deaf and hard-of-hearing
                  people </br></br>
                  In addition, straightforward algorithms have a large vector size, which is
				  frequently equivalent to the number of distinct words in the text. This
                  frequently results in an increase in the amount of time required for the training
				  of classifiers when contrasted with neural network-based methods.</p>
              </div>
              <div class="tab-pane fade show" id="component4">
                <p>Human emotions are sometimes portrayed in a variety of ways, including facial expressions, gestures, tone, and speech. To recognize emotions, these tactics can be employed alone or in various combinations. Although extensive independent study has been conducted for the methodologies listed above, the multimodal approach to determining a user's emotions has received less attention. </br></br>

				Many studies have lately been conducted on emotion identification utilizing user-supplied facial characteristics, tone components, and speech content. Marvin Minsky was the first to suggest programming computers to think in emotional terms. "The question is not whether intelligent computers can have any sentiments," he wrote in his book The Society of Mind, "but whether robots can be brilliant without emotions" [1].   </br></br>

  				Recent research has concentrated on multimodal emotion recognition utilizing visual and audio input. Human emotion expression is frequently multimodal for optimal transmission, covering visual, aural, and literary channels [2]. Even if a person hides his or her emotions for a variety of reasons, physiological signals may objectively indicate an individual's emotional state. As a result, multimodal emotion identification in practical applications has garnered a lot of attention, and the research focus has shifted from a single modality to multimodal emotion detection. D'Mello and Kory conducted research in 2015 that used statistical approaches to examine the accuracy of single-modality and multimodality on different datasets. Multimodal expression recognition outperformed single modality recognition in the studies [3]. According to the McGurk phenomenon, sensory input loss or inaccuracy generates irregularities in the brain's external information processing. As a result, multimodal feature fusion identification has been a popular study area [4].  </br></br>

				The HUMAINE database [5], the Belfast database [6], the large-scale audiovisual database SEMAINE [7], the IEMOCAP emotional database [8], the audiovisual database CUAVE [9], the Acted Facial Expression in the Wild database (AFEW) [10], and the Chinese multimodal emotional database [11] are some of the most widely used multimodal emotion databases.  </br></br>

				I.T. Meftah et al. [12] proposed a multimodal technique for identifying fundamental emotions and mixing emotions such as simulated and disguised emotions utilizing modalities such as facial expressions, vocal expressions, gestures, and bodily movements. To determine five emotional states in humans, Cid et al [13] proposed a multimodal emotion recognition system based on visual and acoustic input. Padgett et al [14] a six-emotion recognition system was introduced. They were able to develop 12 ensembles of face analysis and network models. They employed a multi-layer ensemble model, a kind of neural network. However, this strategy cannot be characterized as a multi-modularity mechanism. </br></br>

				Finally, in recent decades, numerous investigations on systems that can monitor and aid emotions have been done. The number of modalities explored in these experiments, however, is limited, and the multi-modal approach to emotion identification has not been adequately accounted for. Although several ensemble approaches have been used to aggregate predictions obtained from various modalities, individual variation in expressing emotions has not been investigated. One of the key issues we aim to address in our research is user-specific aggregation, which is not addressed. </br></br>
	
				Peer support services (PSSs), on the other hand, are a novel type of intervention that has recently gained favor in mental health systems around the world. PSSs are known to have been used on an ad hoc basis during the moral therapy period for over three centuries [15], albeit on an ad hoc basis. The literature [16] contains a variety of definitions and classifications for PSSs, and Peer support workers have been praised and supported in several publications.  </br></br>

				Social media platforms provide near-constant opportunities to interact and connect with others, regardless of time or location. This on-demand communication ease might be particularly beneficial for strengthening social connection among persons with mental problems who have difficulties participating in intimate interactions. According to Gowen et al  [17], Most young people suffering from significant mental diseases utilize social media to feel less alone. Increased use of social media was associated with increased community participation, as assessed by voting in municipal elections [18].  </br></br>

				Several research have been conducted on the subject of online peer support for people suffering from mental illnesses. A content analysis of comment sections uploaded to YouTube by attendees with severe mental illnesses revealed opportunities to feel less alone, offer more hope, seek assistance, gain insights through mutual reciprocity, and share positive coping strategies for the day-to-day challenges of living with a mental disorder [19]. In a further study conducted in 2009, Chang [20] identified diverse communication patterns in an online psychosis peer-support group in a subsequent study done in 2009. For example, "informational help" on medication use or contacting mental health professionals. "Esteem support" consists of positive affirmations that are encouraging. "Network support" for sharing similar experiences and "emotional support" for demonstrating empathy for a peer's circumstances and providing hope or confidence evolved as distinct sorts of aid [20].  </br></br>

				Moreover, research has shown that social support is essential for mental wellness. However, none of those studies stressed activity recommendations or ideas for things to do with the social support group, which is the second major issue we want to address in our research. </p></br>
              </div>
            </div>
          </div>
          <div class="tab-pane fade" id="Research_Gap">
            <h4 class="mt-2 display-3--title">Research Gap</h4>
            <p>Anyone who desires to design a mental health app endures challenges since it is quite challenging
to deliver the necessary value to the users [48]
Following the analysis of earlier researchers, the following results were discovered.
1. Existing mental health applications have primarily focused on providing generic emotional
support rather than measuring and personalizing based on the user's mental health [49].
2. Emotion models improve learning efficiency, yet the present method of aggregating models is
ensemble learning, which is not user-specific and ignores the assumption that people express
emotions differently.
3. Existing mental health apps do not provide emotional support through a social support group;
rather, they are self-contained. Also, the assumption that social support improves in one's
emotional state is not considered.
4. Existing apps do not use reinforcement learning to personalize group or pair-wise activities
inside the social support network.
However, many researchers have paid attention to other mental health support assets, e.g. High
patient engagement, Self-monitoring features [50], motivational support, psychotherapeutic advice
[51], and Mobile Therapy [52]. </p>
          </div>
          <div class="tab-pane fade" id="Research_Problem">
            <h4 class="mt-2 display-3--title">Research Problem</h4>
            <p>Emotional well-being enables you to function successfully and cope with the demands of daily
              life. It can assist you in attaining your greatest potential. It allows you to collaborate with others
              and make a positive contribution to society. Furthermore, how you feel can have an impact on your
              capacity to carry out daily activities, your relationships, and your overall mental health. Emotional
              wellbeing refers to the ability to cope with life's challenges and to respond to change and adversity.
              Additionally, one of the key studies is that social groups are significant psychological resources
              that can safeguard one's health and well-being.
              We were able to list the issues by considering emotional health and the fact that social groupings
              play a significant part in one's emotional health. The goal of this research is to expose people to
              social contexts to solve the problems outlined below.
              <ul>
                <li>While several existing Psychological support apps adopt a multi-modal approach, they
                  disregard the fact that each person expresses their thoughts differently across these
                  modalities, and thus aggregation should be user-specific.</li>
                  <li>Although there are numerous mental health applications available, only a handful of them
                    monitor emotions and personalize based on the user's mental health.</li>
                  <li>People with mental disorders frequently require help to manage with their illnesses and
                    avoid isolation, which might include social support from family and friends.</li>
                  <li>As a result of "technoference" research, circle of friends, close relatives, or those you love
                    may have neglected, irritated, or pushed away the person who requires the most attention.
                    Smartphones and other technologies have the potential to disrupt our face-to-face social
                    ties.</li>
                  <li>The danger symptoms will not be identified if an individual is in a critical state. As a result,
                    neither the patient's health care practitioner nor the patient's support group, such as parents
                    and friends, will be informed.</li>
                  <li>General therapeutic activities are recommended rather than those tailored to the diagnosed
                    individuals.</li>
                  <li>Current systems do not allow for self-evaluation.</li>
                  <li>The existing applications on the market do not recommend actions that can be done to
                    improve one's mental state and maintain it till it improves.</li>
              </ul></p>
          </div>
          <div class="tab-pane fade" id="Research_Objectives">
            <h4 class="mt-2 display-3--title">Research Objectives</h4>
          </br></br>
            <h4 class="mt-2 display-4--title">Main Objective</h4>
            <p>‘xīnlǐ’ is a cross-platform social media application for Android and iOS.
              Regular users, Supporters, and Mentors all have different duties. Where mentors have additional
              capabilities, such as the ability to observe a user's emotional history based on frequently uploaded
              films.
              It enables users to connect to a personal egocentric network, where they may receive emotional
              assistance from supporters and mentors. Supporters and mentors can also assist many individuals.
              Monitors the subject's mental health condition and displays it as a range. This is not a diagnostic
              tool, but it is utilized by frequent users who require emotional support. </p>
              <h4 class="mt-2 display-4--title">Specific Objective</h4>
            <p><ul>
              <li><p class="font-weight-bold">Predict the emotional status from facial expressions and the posture using the
                emotion video log.</p></li>
                <li><p class="font-weight-bold">Predict the emotional status from tone of the Speech in the user’s video log.</p>
                <p>Create a model based on the tone of the users using the emotion video log. Analyze the model by
                  comparing it to the actual data. The mobile application should be implemented. Predictions may
                  be made by combining the model with a mobile app.</p></li>
                <li><p class="font-weight-bold">Predict the emotional status from semantics (meaning) of the text contained in the
                  user’s video log.
                  </p><p>To accomplish the primary purpose, the proposed system must meet the following four objectives:
                    Predict the emotional state based on the user's tone and the semantics (meaning) of the language
                    contained in the emotion video log.
                    Utilize a simple three-layer neural network to aggregate the three emotional predictions from the
                    preceding three components.
                    Additionally, using reinforcement learning promotes group or couple activities inside the social
                    support network.
                    </p></li>
                <li><p class="font-weight-bold">Aggregate the three modality inputs and, using Reinforcement Learning,
                  recommend activities based on emotion.</p>
                <ol><li>Assemble the models and perform the weighting. </li>
                <li>Create a dataset with the activities that improve the emotional state according to each emotion </li>
                <li>Predict activities according to each emotion using Reinforcement learning </li>
                <li>Find the most appropriate activity with the help of feedback and emotional improvement. </li>
              </ol></li>
            </ul></p>
          </div>
          <div class="tab-pane fade" id="Methodology">
            <h4 class="mt-2 display-3--title">Methodology</h4>
            <ul class="nav nav-pills mb-3 justify-content-center" id="myTab2">
              <li class="nav-item">
                <a href="#component1Meth" class="nav-link active" data-bs-toggle="tab">IT18220520</a>
              </li>
              <li class="nav-item">
                <a href="#component2Meth" class="nav-link" data-bs-toggle="tab">IT18219876</a>
              </li>
              <li class="nav-item">
                <a href="#component3Meth" class="nav-link" data-bs-toggle="tab">IT18257250</a>
              </li>
              <li class="nav-item">
                <a href="#component4Meth" class="nav-link" data-bs-toggle="tab">IT18216424</a>
              </li>
            </ul>
            <div class="tab-content">
              <div class="tab-pane fade show active" id="component1Meth">
                <p>Nonverbal communication includes facial expressions. They
                  are an essential method of human-to-human social information
                  transmission. A person’s facial expression, like their stride
                  and posture, might provide clues about their physical and
                  psychological well-being. Face recognition, feature extraction,
                  and feature classification are the three processes in the standard
                  approach for facial emotion analysis. The Face expression
                  recognition dataset is used to train and test the model. The
                  dataset includes 28,821 annotated photographs in the training
                  set and 7,066 identified images in the test set. Each photo
                  in the Face expression recognition dataset is categorized as
                  one of seven emotions: happy, sad, angry, afraid, surprise,
                  disgust, and neutral. This work used the CNN model and
                  pre-processing techniques to identify/predict facial emotions.
                  The CNN model was chosen because of its effectiveness in
                  extracting non-trivial facial attributes. The CNN
                  model consists of four Convolutional layers, four max-pooling
                  layers, six dropout layers, and two fully connected layers.
                  Hundreds of frames were captured from the video log after
                  the same intervals. The Haar-Cascade algorithm segmented the
                  faces, cropped from the frames, and then put only the images
                  with faces into the model.</p>
                  <img src="images\It1822020\Screenshot 2022-06-13 150252.png" class="rounded mx-auto d-block" alt="System Diagram">
              </div>
              <div class="tab-pane fade show" id="component2Meth">
                <p><b>Aggregation of the three modalities for the multi-modal emotion predictions </b>
				

					<img src="images\IT18219876\876.jpeg" class="rounded mx-auto d-block" alt="System Diagram">			

					While communicating concepts and blending models on an emotional level, it is vital to understand an individual's uniqueness. Each person picks a different manner to convey their emotional state to differing degrees. As a result, a more sophisticated ensemble technique should be able to capture the diverse ways in which each user displays their emotional state. The primary purpose of this component was to capture a wide spectrum of people's emotional responses. The proposed system incorporates a user-specific learning process with the purpose of applying a user-specific weighting to each modality depending on how each individual conveys their emotional state via each model. The suggested strategy for aggregating the findings provides the best weight for each participant. </br></br> 

					We first set a random seed for both the Tensorflow and Numpy libraries so that we can get reproducible when we run them again. The reason for this is that the neural network weights are randomly set at first, therefore if we do not employ this feature at times, the obtained results could be different. Then, using three dense layers and a SoftMax layer, construct a neural network. The weights for computing aggregates will be determined by the parameters in these levels. We must first learn these weights. A Tensorflow primitive has been used to compile the model with loss function and an optimizer. This instructs the model on which optimizer to use for training, what loss function to apply, and what kind of metrices to look for as output. By comparing the model's output to the real output, the loss function is calculated (Predicted vs. Actual). Using the sklearn toolkit, the data set is separated into two sections for neural network training. 80\% of the data will be utilized for training, while the remaining 20\% will be used to verify whether our model has learned. We also use one-hot encoding to encode our integers. This enables us to train a better network and improve accuracy. Finally, using the training data and training labels, the neural network is trained. We then execute the model's evaluation by feeding the prepared test data into the trained model. On the test set, we get around 91.5\% accuracy. This suggests that our model can appropriately aggregate the inputs of the three models approximately in 92\%of the cases. </br></br> 
					
					<b>Personalized activity prediction to improve emotional health </b>
					
					<img src="images\IT18219876\8766.jpeg" class="rounded mx-auto d-block" alt="System Diagram">

					This component oversees anticipating activities based on the user's emotional state. The model learns by considering how the user's emotional state has changed over time and whether the user has completed the exercises previously. By considering these, the system tries to forecast more accurate activities that improve the user's mental state as well as activities that the user is interested in undertaking. </br></br> 

					A dataset was created with the support of a psychologist, psychiatrist, and occupational therapist based on each emotion and the most appropriate activities that may be done to ameliorate that emotional state. The AI model, which incorporated reinforcement learning, was evaluated for predicting the most appropriate activities based on the emotional states of the users. The AI model algorithm determines that the action has had a positive impact on the user's emotional state based on the score or reward received by the user. The Q table was updated with these received data. Each user's Q table, which was built during the registration process, is unique. </br></br> 

					Q-learning methods, which fall under the area of reinforcement machine learning, were employed to implement the model. Python is the programming language of choice for creating machine learning models. The machine learning models were constructed using Python, and the Jupyter notebook development tool was used. Also, as the database we have utilized is a real-time database named Firebase. </br></br> 

					To progress with the procedure, the model requires the user's emotional state and whether or not the user has done the preceding action. The models then carry out the functionality and update the Q-values in accordance with the inputs. The system repeats these processes until the user receives a good Q-table. </br></br> 

					It also forecasts the best activity for each user based on their emotional condition. The model tends to improve forecast accuracy over time. </br></br> </p>
              </div>
              <div class="tab-pane fade show" id="component3Meth">
                <p>Speech emotion recognition is the process of attempting to identify human emotion expressive sates from speech. This is leveraging on the fact that voice typically reveals underline emotion through tone and pitch. This component focuses on how to use the Speech of an individual in mental health prediction. The user may upload the video to the specified mobile application. Only the voice clip was taken from the given video from the system. Then the system turns the speech clip into the variable duration of the acoustic signal and detects users’ voices using the Speech emotion Recognition System. The Spoken emotion Recognition System identifies user voices after converting the speech clip into a variable duration audio wave. From that voice fluctuation, the system evaluates users’ emotions, pitch loudness, and rhythm, and analyzes those experiences utilizing tone analysis and emotion analysis in a unique way. </br></br> 
 
					We employed a CNN model to identify/predict speech emotions. The reason for selecting the CNN model was its effectiveness in extracting features. The CNN model used had twelve layers, including the convolutional, max-pooling, dropout and fully connected layers. The RAVDESS dataset is used for training and testing the implemented model. The RAVDESS dataset is an audio data set with 12 actors and 12 actresses. Under the preparation and pre-processing of the dataset, the audio was categorized into the seven emotional categories that are considered. </br></br> 

					Since our dataset is not that big, we modified our existing data set and generated synthetic data from audio. We applied noise injection, shifting time, and pitch scaling to generate synthetic data for audio. The extraction of characteristics was critical for evaluating and discovering relationships between various items. The data provided by audio cannot be interpreted directly by the models, thus we needed to turn it into a comprehensible format, for which we utilized feature extraction. To do this, we used a special library in python called Librosa. In this work, we have selected feature extraction to MFCC (Mel Frequency Cepstral Coefficients) and Mel spectrogram. We used those Mel spectrograms to train and test the model. </br></br> 
			</p>
			<img src="images\IT18257250\520.jpeg" class="rounded mx-auto d-block" alt="System Diagram">
			
              </div>
              <div class="tab-pane fade show" id="component4Meth">
                <p>Emotion recognition is a dynamic methodology that is

					based on a person’s emotional state, meaning that the emo-
					tions connected with each person’s activities are unique. This

					component’s main purpose is to demonstrate how semantics
					may be used to accurately measure an individual’s emotional
					state. The user is urged to upload/record a video diary on a
					regular basis (daily, twice a day, etc.) to convey their feelings,
					which is then shared with support networks. Based on the
					semantics (meaning) of the words in the video, predict the
					user’s emotional state. To do so, the voice must first be
					transformed into audio. The audio has been converted into
					text. If the audio isn’t in English, the Google Translate API is
					used to convert it to English. An API was then used to convert
					the audio to text. Following that, Natural Language Processing
					and emotional analysis were used to make predictions. The six
					primary emotions (happiness, surprise, fear, contempt, rage,

					and sorrow) and neurobiology are the core emphasis of pre-
					diction.</br></br> ISEAR, Emotional Stimulation, and Daily-Dialog data

					sets have been used to create the data set. ISEAR is a collection
					of 7665 annotated sentences expressing the emotions of joy,
					sadness, fear, anger, guilt, disgust, and shame obtained from
					cross-cultural studies in 37 countries. emotion-stimulus data
					set is developed from FrameNets’ annotated data for emotion

					lexical unit. Contains 1594 emotion-labelled sentences. Daily-
					Dialog data set contains 13118 Dialogues extracted from

					conversations and annotated for happiness, sadness, anger,
					disgust, fear, surprise, and others. After combined these three
					data sets remove unnecessary fields of emotions, a new data
					set have been created. To achieve prediction goal LSTM (Long
					Short-Term Memory) Algorithm is used with the help of
					python and its libraries such as Tensorflow and Keras .LSTMs
					were specifically designed to solve the issue of long-term
					dependency. They don’t have to work hard to remember things
					for lengthy periods of time; it happens naturally to them.</br></br>

					<img src="images\IT1826424\Picture1.jpg" class="rounded mx-auto d-block" alt="System Diagram">

					In the figure above, each square sends a whole vector from
					one node’s output to the inputs of others.The first layer is the
					embedded layer, which represents each word with 100 length
					vectors. Then the spatial dropout 1D layer performs variationals
					dropout in NLP models. Third Layer is the LSTM layer with
					100 memory units. The output layer must create 7 output
					values for each class. Here as the Activation function softmax
					is used. Since this is a multi-class classification problem,
					categorical crosstropy is used as the loss function.</p>
              </div>
            </div>
          </div>
          <div class="tab-pane fade" id="Technologies">
            <h4 class="mt-2 display-3--title">Tools & Technologies</h4>
            <p>
              <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-arrow-right-square-fill" viewBox="0 0 16 16">
                <path d="M0 14a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H2a2 2 0 0 0-2 2v12zm4.5-6.5h5.793L8.146 5.354a.5.5 0 1 1 .708-.708l3 3a.5.5 0 0 1 0 .708l-3 3a.5.5 0 0 1-.708-.708L10.293 8.5H4.5a.5.5 0 0 1 0-1z"/>
              </svg> Google Colab</br>
              <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-arrow-right-square-fill" viewBox="0 0 16 16">
                <path d="M0 14a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2H2a2 2 0 0 0-2 2v12zm4.5-6.5h5.793L8.146 5.354a.5.5 0 1 1 .708-.708l3 3a.5.5 0 0 1 0 .708l-3 3a.5.5 0 0 1-.708-.708L10.293 8.5H4.5a.5.5 0 0 1 0-1z"/>
              </svg>Python</br>
            </p>
          </div>
        </div>
      </div>
  </section>

    <!-- //////////////////////////////////////////////////////////////////////////////////////////////
                             START SECTION - Milestone
////////////////////////////////////////////////////////////////////////////////////////////////////-->

<section id="Milestones" class="Milestones">
  <div class="container">
    <div class="row text-center mt-5">
      <h1 class="display-3 fw-bold text-capitalize">Milestones</h1>
      <div class="heading-line"></div>
    </div>
    <div class="container">
      <div class="timeline">
          <div class="timeline-row">
            <div class="timeline-time">
              28 June<small>2022</small>
            </div>
            <div class="timeline-content">
              <i class="icon-attachment"></i>
              <h4 class="display-3--title">Project Proposal</h4>
              <p>Proposal presentation and the proposal report submission.</p>
              
              
            </div>
          </div>
      
          <div class="timeline-row">
            <div class="timeline-time">
              28 June<small>2022</small>
            </div>
            <div class="timeline-content">
              <i class="icon-code"></i>
              <h4 class="display-3--title">Progress Presentation I</h4>
              <p>
                50% progress presentation of the research project.
              </p>
            </div>
          </div>
      
          <div class="timeline-row">
            <div class="timeline-time">
              28 June<small>2022</small>
            </div>
            <div class="timeline-content">
              <i class="icon-turned_in_not"></i>
              <h4 class="display-3--title">Progress Presentation II</h4>
              <p>90% progress presentation of the research project.</p>
              
            </div>
          </div>
      
          <div class="timeline-row">
            <div class="timeline-time">
              28 June<small>2022</small>
            </div>
            <div class="timeline-content">
              <i class="icon-directions"></i>
              <h4 class="display-3--title">Demonstration</h4>
              <p>Submission and presentation of the camera-ready research poster.</p>
              
            </div>
          </div>
      
          <div class="timeline-row">
            <div class="timeline-time">
              28 June<small>2022</small>
            </div>
            <div class="timeline-content">
              <i class="icon-change_history"></i>
              <h4 class="display-3--title">Final Assesment</h4>
              <p class="no-margin">Submission of final reports and the final presentation of the research.</p>
              
            </div>
          </div>
      
          <div class="timeline-row">
            <div class="timeline-time">
              28 June<small>2022</small>
            </div>
            <div class="timeline-content">
              <i class="icon-code"></i>
              <h4 class="display-3--title">Viva</h4>
              <p>
                Final viva of the research project with an externel panel member.
              </p>
              
            </div>
          </div>
      
        </div>
      </div>
</section>


    <!-- ///////////////////////////////////////////////////////////////////////////////////////////////////
                              START SECTION - Presentation 
//////////////////////////////////////////////////////////////////////////////////////////////////////-->

<section id="Presentations" class="Presentations">
  <div class="container">
    <div class="row text-center mt-5">
      <h1 class="display-3 fw-bold text-capitalize">Presentation</h1>
      <div class="heading-line"></div>
      <p class="lead">
        All the Presentation of the project.
      </p>
      <div class="container justify-content-center">
        <div class="card-group justify-content-center">
          <div class="card">
            <img class="card-img-top" src="images\It1822020\m2.png" alt="Bologna">
            <div class="card-body">
              <h4 class="card-title">Proposal Presentation</h4>
              <p class="card-text">Click <strong>below</strong> to view</p>
              <a href="https://docs.google.com/presentation/d/18fzn1LTWm1-KJvTU2tc2r0pOqiyOKF_o/edit?usp=sharing&ouid=104193131732890187046&rtpof=true&sd=true" class="card-link text-danger">view</a>
            </div>
          </div>
          <div class="card">
            <img class="card-img-top" src="images\It1822020\m2.png" alt="Bologna">
            <div class="card-body">
              <h4 class="card-title">Progress Presentation 1</h4>
              <p class="card-text">Click <strong>below</strong> to view</p>
              <a href="https://docs.google.com/presentation/d/1g0XkoKBfMWFEQt1ut6acPrgNdIhJaZiE/edit?usp=sharing&ouid=104193131732890187046&rtpof=true&sd=true" class="card-link text-danger">view</a>
            </div>
          </div>
          <div class="card">
            <img class="card-img-top" src="images\It1822020\m2.png" alt="Bologna">
            <div class="card-body">
              <h4 class="card-title">Progress Presentation 2</h4>
              <p class="card-text">Click <strong>below</strong> to view</p>
              <a href="https://docs.google.com/presentation/d/16dQEP0hwykQ6zQAv63VqRI9HtgTidb9d/edit?usp=sharing&ouid=104193131732890187046&rtpof=true&sd=true" class="card-link text-danger">view</a>
            </div>
          </div>
        </div>
        </div>

    </div>

</section>


    <!-- //////////////////////////////////////////////////////////////////////////////////////////////
                             START SECTION - Documentation
////////////////////////////////////////////////////////////////////////////////////////////////////-->

<section id="Documentation" class="Documentation">
  <div class="container">
    <div class="row text-center mt-5">
      <h1 class="display-3 fw-bold text-capitalize">Documentation</h1>
      <div class="heading-line"></div>
      <p class="lead">
        All the documentations of the project.
      </p>
    </div>

    <div class="container justify-content-center">
      <div class="card-group justify-content-center">
        <div class="card">
          <img class="card-img-top" src="images\It1822020\m1.png" alt="Bologna">
          <div class="card-body">
            <h5 class="card-title">PROJECT CHARTER</h5>
            <p class="card-text">Click <strong>below</strong> to view</p>
            <a href="#" class="card-link text-danger">view</a>
          </div>
        </div>
        <div class="card">
          <img class="card-img-top" src="images\It1822020\m1.png" alt="Bologna">
          <div class="card-body">
            <h5 class="card-title">PROPOSAL</h5>
            <a href="https://docs.google.com/document/d/176_6uAshVQ_GCRMmeGkuh2LxKB9DkBlp/edit?usp=sharing&ouid=104193131732890187046&rtpof=true&sd=true" class="card-link text-danger">IT18220520</a></br>
            <a href="https://docs.google.com/document/d/1Lxcv7u4oty7KSQOufsOfxol_XVbh39Kj/edit?usp=sharing&ouid=104193131732890187046&rtpof=true&sd=true" class="card-link text-danger">IT18219876</a></br>
            <a href="https://docs.google.com/document/d/1Dt4PTHo9PAdBxbzr73ygkV_nZCBbAlH-/edit?usp=sharing&ouid=104193131732890187046&rtpof=true&sd=true" class="card-link text-danger">IT18257250</a></br>
            <a href="https://docs.google.com/document/d/1RIiA6DNJxXB540rewLrNUlrRujry7dWw/edit?usp=sharing&ouid=104193131732890187046&rtpof=true&sd=true" class="card-link text-danger">IT18216424</a>
          </div>
        </div>
        <div class="card">
          <img class="card-img-top" src="images\It1822020\m1.png" alt="Bologna">
          <div class="card-body">
            <h5 class="card-title">RESEARCH LOG BOOK</h5>
            <a href="#" class="card-link text-danger">IT18220520</a></br>
            <a href="#" class="card-link text-danger">IT18219876</a></br>
            <a href="#" class="card-link text-danger">IT18257250</a></br>
            <a href="#" class="card-link text-danger">IT18216424</a>
          </div>
        </div>
        <div class="card">
          <img class="card-img-top" src="images\It1822020\m1.png" alt="Bologna">
          <div class="card-body">
            <h5 class="card-title">STATUS DOCUMENT I</h5>
            <a href="https://drive.google.com/file/d/1bWDPQl_P70prEQWBk_k0yYuktpBEG5cj/view?usp=sharing" class="card-link text-danger">IT18220520</a></br>
            <a href="https://drive.google.com/file/d/1VDPt_-g2v5_vFImEBsSa3Ue-laVGZGwe/view?usp=sharing" class="card-link text-danger">IT18219876</a></br>
            <a href="https://drive.google.com/file/d/1WNLdGut7-QrXUPwAjnyGYFD32EFqt3Qx/view?usp=sharing" class="card-link text-danger">IT18257250</a></br>
            <a href="https://drive.google.com/file/d/1RkCv7p9GCKlpxnL-QtMys_C6djPw_gIB/view?usp=sharing" class="card-link text-danger">IT18216424</a>
          </div>
        </div>
        <div class="card">
          <img class="card-img-top" src="images\It1822020\m1.png" alt="Bologna">
          <div class="card-body">
            <h5 class="card-title">STATUS DOCUMENT II</h5>
            <a href="#" class="card-link text-danger">IT18220520</a></br>
            <a href="#" class="card-link text-danger">IT18219876</a></br>
            <a href="#" class="card-link text-danger">IT18257250</a></br>
            <a href="#" class="card-link text-danger">IT18216424</a>
          </div>
        </div>
            <div class="card">
              <img class="card-img-top" src="images\It1822020\m1.png" alt="Bologna">
              <div class="card-body">
                <h5 class="card-title">RESEARCH PAPER</h5>
                <p class="card-text">Click <strong>below</strong> to view</p>
                <a href="https://drive.google.com/file/d/1F97ZoyI3P8D9I8pjluhgZMqXbzYWGlDY/view?usp=sharing" class="card-link text-danger">view</a>
              </div>
            </div>
        <div class="card">
          <img class="card-img-top" src="images\It1822020\m1.png" alt="Bologna">
          <div class="card-body">
            <h5 class="card-title">FINAL REPORTS</h5>
            <a href="https://drive.google.com/file/d/1wH4JkzKxv16z75i-cM_2QH7rkLmydoYL/view?usp=sharing" class="card-link text-danger">IT18220520</a></br>
            <a href="https://drive.google.com/file/d/1uv5qWZJiYCfZjDWIBPu2RUN5uEBRXFJd/view?usp=sharing" class="card-link text-danger">IT18219876</a></br>
            <a href="https://drive.google.com/file/d/1sWKXH3ckbaMRYMxQZO2AtBcYFZwyOVt_/view?usp=sharing" class="card-link text-danger">IT18257250</a></br>
            <a href="https://drive.google.com/file/d/1YGEb2KXVktXMgRD-73OlmBujkM_iw-f5/view?usp=sharing" class="card-link text-danger">IT18216424</a></br>
            <a href="https://drive.google.com/file/d/1mtMoaLBog_ak0LwkyRlssEdOPlM3c9xF/view?usp=sharing" class="card-link text-info">Group report</a>
          </div>
        </div>
      </div>
    
    </div>
</section>

  <!-- //////////////////////////////////////////////////////////////////////////////////////////////
                             START SECTION - Meet Our Team
////////////////////////////////////////////////////////////////////////////////////////////////////-->

<section id="Team" class="Team">
  <div class="container">
    <div class="row text-center mt-5">
      <h1 class="display-3 fw-bold text-capitalize">Meet our Team</h1>
      <div class="heading-line"></div>
    </div>
  <div class="row justify-content-around">
    <div class="col-lg-4 col-md-6">
      <div class="portfolio-box shadow">
        <img src="images\It1822020\p1.png" alt="portfolio 1 image" title="portfolio 1 picture"
          class="img-fluid">
        <div class="portfolio-info">
          <div class="caption">
            <h4>Dharshana Kasthurirathna</h4>
            <div>
              <p>Supervisor</p>
            </div>
            <div class="row-md-6">
              <p><span class="text-warning">dharshana.k@sliit.lk</span></p>
            </div>
          </div>
        </div>
      </div>
    </div>
    <div class="col-lg-4 col-md-6">
      <div class="portfolio-box shadow">
        <img src="images\It1822020\p2.png" alt="portfolio 1 image" title="portfolio 1 picture"
          class="img-fluid">
        <div class="portfolio-info">
          <div class="caption">
            <h4>Dimuth Adeepa</h4>
            <div>
              <p>Co-Supervisor</p>
            </div>
            <div class="row-md-6">
              <p><span class="text-warning">dimuth.a@sliit.lk</span></p>
            </div>
          </div>
        </div>
      </div>
    </div>
    <div class="col-lg-4 col-md-6">
      <div class="portfolio-box shadow">
        <img src="images\It1822020\pro pic.jpg" alt="portfolio 1 image" title="portfolio 1 picture"
          class="img-fluid">
        <div class="portfolio-info">
          <div class="caption">
            <h4>Didula Nanayakkara</h4>
            <div>
              <p>Member</p>
            </div>
            <div class="row-md-6">
              <p><span class="text-warning">It18220520@my.sliit.lk</span></p>
            </div>
          </div>
        </div>
      </div>
    </div>
    <div class="col-lg-4 col-md-6">
      <div class="portfolio-box shadow">
        <img src="images\IT18219876\pic1.png" alt="portfolio 1 image" title="portfolio 1 picture"
          class="img-fluid">
        <div class="portfolio-info">
          <div class="caption">
            <h4>Shadini Kalansooriya</h4>
            <div>
              <p>Member</p>
            </div>
            <div class="row-md-6">
              <p><span class="text-warning">it18219876@my.sliit.lk</span></p>
            </div>
          </div>
        </div>
      </div>
    </div>
    <div class="col-lg-4 col-md-6">
      <div class="portfolio-box shadow">
        <img src="images\IT18257250\pic2.jpeg" alt="portfolio 1 image" title="portfolio 1 picture"
          class="img-fluid">
        <div class="portfolio-info">
          <div class="caption">
            <h4>Chamudi Sasanka</h4>
            <div>
              <p>Member</p>
            </div>
            <div class="row-md-6">
              <p><span class="text-warning">it18257250@my.sliit.lk</span></p>
            </div>
          </div>
        </div>
      </div>
    </div>
    <div class="col-lg-4 col-md-6">
      <div class="portfolio-box shadow">
        <img src="images\It1826424\IMG_7668.jpg" alt="portfolio 1 image" title="portfolio 1 picture"
          class="img-fluid">
        <div class="portfolio-info">
          <div class="caption">
            <h4>Aloka Kaluarachchi</h4>
            <div>
              <p>Member</p>
            </div>
            <div class="row-md-6">
              <p><span class="text-warning">it18216424@my.sliit.lk</span></p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section></br></br>


  <!-- /////////////////////////////////////////////////////////////////////////////////////////////////////////
              START SECTION - Contact
/////////////////////////////////////////////////////////////////////////////////////////////////////////-->
  <section id="contact" class="contact">
    <div class="container">
      <div class="row text-center">
        <h1 class="display-3 fw-bold text-capitalize">Contact Us</h1>
        <div class="heading-line"></div>
        <p class="lh-lg">
          
        </p>
      </div>

      <!-- START THE CTA CONTENT  -->
      <div class="row text-white justify-content-around">
        <div class="contact3 py-5">
          <div class="row no-gutters">
            <div class="container">
              <div class="row">
                <div class="col-lg-6">
                  <div class="card-shadow">
                    <img src="images\It1822020\c1.jpeg" class="img-fluid">
                  </div>
                </div>
                <div class="col-lg-6">
                  <div class="contact-box ml-3">
                    <h1 class="display-3--title font-weight-light mt-2">Quick Contact</h1>
                    <form class="mt-4">
                      <div class="row">
                        <div class="col-lg-12">
                          <div class="form-group mt-2">
                            <input class="shadow form-control form-control-lg" type="text" placeholder="name">
                          </div>
                        </div>
                        <div class="col-lg-12">
                          <div class="form-group mt-2">
                            <input class="shadow form-control form-control-lg" type="email" placeholder="email address">
                          </div>
                        </div>
                        <div class="col-lg-12">
                          <div class="form-group mt-2">
                            <input class="shadow form-control form-control-lg" type="text" placeholder="phone">
                          </div>
                        </div>
                        <div class="col-lg-12">
                          <div class="form-group mt-2">
                            <textarea class="shadow form-control form-control-lg" rows="3" placeholder="message"></textarea>
                          </div>
                        </div>
                        <div class="text-center d-grid mt-1">
                          <button type="submit" class="btn btn-primary rounded-pill pt-3 pb-3">
                            submit
                            <i class="fas fa-paper-plane"></i>
                          </button>
                        </div>
                      </div>
                    </form>
                  </div>
                </div>
                <div class="col-lg-12">
                  <div class="card mt-4 border-0 mb-4">
                    <div class="row text-dark justify-content-around">
                      <div class="col-lg-4 col-md-4">
                        <div class="card-body d-flex align-items-center c-detail pl-0 justify-content-center">
                          <div class="p-5 mr-5 align-self-center">
                            <img src="https://www.wrappixel.com/demos/ui-kit/wrapkit/assets/images/contact/icon1.png">
                          </div>
                          <div class="">
                            <h6 class="font-weight-medium display-3--title">Address</h6>
                            <p class="">SLIIT Malabe Campus, New Kandy Rd
                              <br> Malabe 10115 </p>
                          </div>
                        </div>
                      </div>
                      <div class="col-lg-4 col-md-4">
                        <div class="card-body d-flex align-items-center c-detail justify-content-center">
                          <div class="p-5 mr-5 align-self-center">
                            <img src="https://www.wrappixel.com/demos/ui-kit/wrapkit/assets/images/contact/icon2.png">
                          </div>
                          <div class="">
                            <h6 class="font-weight-medium display-3--title">Phone</h6>
                            <p class="">0117 544 801</p>
                          </div>
                        </div>
                      </div>
                      <div class="col-lg-4 col-md-4">
                        <div class="card-body d-flex align-items-center c-detail justify-content-center">
                          <div class="p-5 mr-5 align-self-center">
                            <img src="https://www.wrappixel.com/demos/ui-kit/wrapkit/assets/images/contact/icon3.png">
                          </div>
                          <div class="">
                            <h6 class="font-weight-medium display-3--title">Email</h6>
                            <p class="p-0 mr-5 align-self-center">
                              shadinikalansooriya98@gmail.com
							  <br> alokakaluarachchi@gmail.com
							  <br> chamudisasanka123@gmail.com,
							
                              <br> sasankadidula97@gmail.com
                            </p>
                          </div>
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
        
      </div>
    </div>
  </section>

  <!-- ///////////////////////////////////////////////////////////////////////////////////////////
                           START SECTION - Footer 
///////////////////////////////////////////////////////////////////////////////////////////////-->
  <footer class="footer">

    <!-- START THE SOCIAL MEDIA CONTENT  -->
    <div class="footer-sm" style="background-color: #212121;">
      <div class="container">
        <div class="row py-4 text-center text-white">
          <div class="col-lg-5 col-md-6 mb-4 mb-md-0">
            connect with us on social media
          </div>
          <div class="col-lg-7 col-md-6">
            <a href="#"><i class="fab fa-facebook"></i></a>
            <a href="#"><i class="fab fa-twitter"></i></a>
            <a href="#"><i class="fab fa-github"></i></a>
            <a href="#"><i class="fab fa-linkedin"></i></a>
            <a href="#"><i class="fab fa-instagram"></i></a>
          </div>
        </div>
      </div>
    </div>


    <!-- START THE COPYRIGHT INFO  -->
    <div class="footer-bottom pt-5 pb-5">
      <div class="container">
        <div class="row text-center text-white">
          <div class="col-12">
            <div class="footer-bottom__copyright">
              &COPY; Copyright 2022 <a href="#">HighwayToHell</a> | Created by <a href="#"
                target="_blank">Didula</a><br><br>

              Distributed by <a href="#" target="_blank">Xinli</a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- BACK TO TOP BUTTON  -->
  <a href="#" class="shadow btn-primary rounded-circle back-to-top">
    <i class="fas fa-chevron-up"></i>
  </a>




  <script src="assets/vendors/js/glightbox.min.js"></script>

  <script type="text/javascript">
    const lightbox = GLightbox({
      'touchNavigation': true,
      'href': 'https://youtu.be/aGv0RSSwlKA',
      'type': 'video',
      'source': 'youtube', //vimeo, youtube or local
      'width': 900,
      'autoPlayVideos': 'true',
    });

  </script>
  <script>
    $(document).ready(function () {
      $("#myTab a:last").tab("show"); // show last tab
    });
  </script>
  <script src="assets/js/bootstrap.bundle.min.js"></script>
</body>

</html>